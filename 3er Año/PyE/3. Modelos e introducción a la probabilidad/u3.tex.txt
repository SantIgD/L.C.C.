\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{vmargin}
\usepackage{graphicx}
\graphicspath{ {images/} }
\setpapersize{A4}
\setmargins{2.5cm}       % margen izquierdo
{1.5cm}                        % margen superior
{16.5cm}                      % anchura del texto
{23.42cm}                    % altura del texto
{10pt}                           % altura de los encabezados
{1cm}                           % espacio entre el texto y los encabezados
{0pt}                             % altura del pie de página
{2cm}           
\begin{document}
\begin{titlepage}
\centering
{\bfseries\LARGE Universidad Nacional de Rosario \par}
\vspace{0,5cm}
{\scshape\Large Facultad de Ciencias Exactas, Ingeniería y Agrimensura \par}
\vspace{2cm}
{\scshape\Huge Probabilidad y Estadística\par}
\vspace{3cm}
{\itshape\Huge Unidad 3 \par}
\vfill
{\Large Autor del resumen:\par}
\vspace{1 cm}
{\Large Charles Chaplin \par}
\vfill
{\Large Julio 2020 \par}
\end{titlepage}

\newpage
\section{Modelos y Fenómenos}
\vspace{0,5cm}
{\normalsize Es muy importante distinguir entre el fenómeno observable en sí mismo y el modelo matemático para dicho fenómeno. No influimos sobre lo que observamos; sin embargo, al elegir un modelo, sí podemos aplicar nuestro juicio crítico.}

\vspace{0,5cm}

{Los modelos que estudiaremos son el probabilístico o estocástico y el determinístico. El modelo determinístico es aquel que estipula que las condiciones bajo las cuales se verifica un experimento determinan el resultado del mismo.}

\vspace{0,5cm}

{\bf Ejemplo:}

\vspace{0,5cm}

{Colocamos una batería a un circuito simple, el modelo matemático que posiblemente describiría el flujo observable de corriente sería $I = \frac{E}{R}$ , que es la Ley de Ohm. Este modelo predice el valor de I tan pronto como se dan las condiciones E y R. En otras palabras, si se repitiese el experimento anterior cierto número de veces, empleando cada vez el mismo circuito (esto es, manteniendo fijos E y R) posiblemente hubiéramos esperado obtener el mismo valor de I.}
\vspace{0,5cm}

{\footnotesize\em El resultado de medir I se puede ver afectador por factores como el alambre conductor, la batería, el amperímetro, entre otras posibilidades. Cualquier desviación que pudiese ocurrir sería tan pequeña que la mayor parte de los objetivos de la descripción anterior (el modelo) se cumplirían. }

\vspace{0,5cm}
{En la naturaleza hay muchos ejemplos de experimentos para los cuales el modelo deterministico es apropiado. Entre estos están las leyes gravitacionales (describen con precisión lo que le sucede a un cuerpo que cae enciertas condiciones); o las leyes de Kepler, que nos indican el comportamiento de los planetas.  }
\vspace{0,7cm}

{Dicho todo lo anterior una definición de modelo determinístico sería:}
\vspace{0,5cm}

{\bf El modelo determinístico señala que las condiciones en las cuales se verifican ciertos fenómenos determinan el valor de ciertas variables observables: la magnitud de la velocidad, el área recorrida durante cierto periodo de tiempo, etc.}
\vspace{1cm}

{Existen otros fenómenos para los cuales el modelo determinístico no es suficiente para una correcta investigación. Por ejemplo, dado un trozo de material radioactivo que emite particulas $\alpha$ podríamos observar la cantidad de partículas en un intervalo de tiempo gracias a un medidor. Es evidente que no podemos predecir exactamente el número de partículas emitidas, aunque sepamos la forma exacta, la dimensión, la composición química y la masa del objeto que se considera. Así no parece haber un modelo determinista razonable que nos indique el número de partículas emitidas, digamos n, como una función de varias características propias de la fuente de radiactividad. En su lugar, debemos considerar un \bf {modelo probabilístico}. }
\vspace{0,5cm}

{\footnotesize Más ejemplos en la página 18 -del total del pdf- }
\vspace{0,5cm}

{{\bf Conclusión}: en un modelo determinista se supone que el resultado real (sea numérico o de otra especie) está definido por las condiciones en las cuales se efectúa el experimento o procedimiento. En un modelo no determinista, sin embargo, las condiciones experimentales sólo determinan el comportamiento probabilístico (más específicamente, la distribución probabilística) de los resultados observables.
En otras palabras, en un modelo determinista, utilizamos “consideraciones específicas” para predecir el resultado, mientras que en un modelo probabilístico usamos la misma clase de consideraciones que para especificar una distribución de probabilidades.}

\newpage

\section{Introducción a la probabilidad}

\vspace{0,5cm}


\subsection{Experimentos aleatorios}
\vspace{0,5cm}
{\large\bf Experimentos "aleatorios" $\sim$ "no deterministas" $\sim$ "Experimentos estudiados con modelos probabilísticos". Ejemplos: }
\vspace{0,5cm}

\begin{itemize}
    \item $E_1$    : Se lanza un dado y se observa el número que aparece en la cara superior.
    \item $E_2$    : Se lanza una moneda cuatro vcccs y se cuenta el número total de caras obtenidas.
    \item $E_3$    : Se lanza una moneda cuatro veces y se observa la sucesión de caras y sellos obtenidos.
    \item $E_4$    : Se fabrican artículos en una línea de producción y se cuenta el número de artículos defectuosos producidos en un periodo de 24 horas.
    \item $E_5$    : El ala de un aeroplano se arma con un gran número de remaches. Se cuenta el número de remaches defectuosos.
    \item $E_6$    : Se fabrica una bombilla. Luego se prueba su duración conectandola en un portalámparas y se anota el tiempo transcurrido (en horas) hasta que se quema.
    \item $E_7$    : En un lote de 10 artículos hay 3 defectuosos. Se elige un artículo después de otro (sin sustituir el articulo elegido) hasta que se obtiene el último artículo defectuoso. Se cuenta el número total de artículos sacados del lote.
    \item $E_8$    : Se fabrican artículos hasta producir 10 no defectuosos. Se cuenta el número total de artículos manufacturados.
    \item $E_9$    : Se lanza un proyectil. Después de un tiempo determinado t, se anotan los tres componentes de la velocidad $v_x$, $v_y$, $v_z$.
    \item $E_{10}$ : Se observa un proyectil recién lanzado en tiempos, $t_1$, $t_2$,..., $t_n$.En cada oportunidad se anota la altura del proyectil sobre el suelo.
    \item $E_{11}$ : Medir la resistencia a la tensión de una barra de acero.
    \item $E_{12}$ : De una urna que contiene sólo esferas negras, se escoge una esfera y se anota su color.
    \item $E_{13}$ : Un termógrafo marca la temperatura continuamente en un periodo de 24 horas. En un sitio y en una fecha señalados, "leer" dicho termógrafo.
    \item $E_{14}$ : En la situación descrita en $E_{13}$ se anotan las temperaturas mínima y máxima, $x$ y $y$ del periodo de 24 lloras considerado.
\end{itemize}
\vspace{0,5cm}

{\large Características de un experimento aleatorio\footnote{Al describir los diversos experimentos, hemos especificado no
sólo el procedimiento que se realiza, sino también lo que estamos interesados en observar (veremos en otra unidad que esto se relaciona estrechamente con las variables aleatorias). }:}
\begin{itemize}
    \item Es posible repetir un experimento de forma indefinida sin cambiar escencialmente las condiciones.\footnote{Si pensamos en modelos determinísticos y en una función que nos devuelve el valor que nos es de interés, los argumentos de esta función serían las condiciones}.
    \item Podemos describir el conjunto de todos los resultados posibles del experimento.
    \item Como el experimento se repita un gran número de veces aparecerá un patrón definido o regularidad. Esta regularidad hace posible la construcción de un modelo matemático con el cual lo analizaremos. 
\end{itemize}
\newpage




\subsection{El espacio muestral}
\vspace{0,5cm}

{{\bf Definición} Dado un experimento $\epsilon$ definimos su espacio muestral asociado como el conjunto de todos los posibles resultados de $\epsilon$. Usualmente se le designa la letra S.}
\vspace{0,5cm}

{Consideremos cada uno de los experimentos anteriores y describamos el espacio muestral de cada uno. El espacio muestral $S_i$ se referirá
al experimento $E_i$. }


\begin{itemize}
    \item $S_1$     : \{1,2,3,4,5,6\}
    \item $S_2$     : \{0,1,2,3,4\}
    \item $S_3$     : \{Todas las posibles sucesiones de la forma $a_1,a_2,a_3,a_4$, donde cada $a_i$ = Cara o Cruz de acuerdo al i-ésimo lanzamiento\}
    \item $S_4$     : \{0,1,...,K; Siendo K la máxima capacidad de producción\}.
    \item $S_5$     : \{0,1,...,K; Siendo K la cantidad total de remaches\}.
    \item $S_6$     : \{t $|$ $t \geq 0$\}
    \item $S_7$     : \{3,4,...,10\}
    \item $S_8$     : \{10,11,...\}
    \item $S_9$     : \{x,y,z $|$ $x,y,z \in \Re$\}
    \item $S_{10}$ : \{$h_1,h_2,...,h_n$ $|$ $h_i\ \geq\ 0;\ \forall \ i\ \in \ {1,...,10}$\}
    \item $S_{11}$ : \{K $|$ $K\ \geq \ 0$\}
    \item $E_{12}$ : \{Esfera Negra\}
    \item $E_{13}$ :  Este espacio muestral es el más importante de los que aquíconsideramos. Prácticamente debemos suponer que la temperatura en cierta localidad específica nunca puede subir o bajar con relación a ciertos valores, digamos M y m. Fuera de esta restricción, debemos admitir la posibilidad de que aparezca cualquier gráfica con determinadas características. Es posible que ésta no tenga saltos (esto es, representará una función continua). Además, la gráfica tendrh ciertas características (le suavidad que pueden resumirse en forma matemática al decir que la gráfica representa una función diferenciable. Así, finalmente podemos enunciar ue el espacio muestral es: \{f $|$ f una función diferenciable, que satisface m $\leq$ f(t) $\leq$ M, $\forall$ t\}

    \item $E_{14}$ : \{$(x,y)$ $|$ m $\leq$ z $\leq$ y $\leq$ M\}. Es decir, $S_{14}$ consta de todos
los puntos que están sobre y en un triángulo en el plano bidimensional z, y. 
\end{itemize}
\vspace{0,5cm}

{Cuando buscamos describir un espacio muestral debemos tener una idea muy clara de lo que medimos u observamos. Por lo tanto deberíamos hablar de "un" espacio muestral asociado con un experimento y no de "el" espacio muestral (notar diferencia entre $S_2 \ y \ S_3$).}
\vspace{0,5cm}

{De acuerdo al tamaño muestral, el espacio muestral puede ser:}
\vspace{1cm}

{$S$ = $\left \{ \begin{array}{lcc}
             Finito    & si &   |S| \ = \ K \ (S_1) \\
             \\ Infinito \ numerable  &  si & |S| \ = \ \aleph_0 \ (S_8) \\
             \\ Infinito \ no \ numerable &  si  & |S| \ = \ \aleph_1 \ (S_6)
             \end{array}
   \right.$
   }
\newpage

\subsection{Eventos}
\vspace{0,5cm}

{{\bf Definición} Dado un experimento $\epsilon$ y sea S{\footnote{S es un evento, así como $\emptyset$ también lo es}} su espacio muestral asociado, definimos un evento A como un conjunto de posibles resultados de $\epsilon$. Es decir, un evento es un subconjunto del espacio muestral asociado a $\epsilon$.}
\vspace{0,5cm}

{Consideremos algunos de los experimentos anteriores y describamos algún evento de ellos. El evento $A_i$ se referirá
al experimento $E_i$. }

\begin{itemize}
    \item $A_1$    : Un número par ocurre; $A_1$ = \{2,4,6\}
    \item $A_2$    : $A_2$ = \{2\}; es decir que ocurren 2 caras 
    \item $A_3$    : $A_3$ = \{CCCC, CCCS, CCSC, CSCC, SCCC\}; es decir que salen más Caras que Cruz(S)
    \item $A_4$    : $A_4$ = \{0\}; es decir que todos los artículos fueron no defectuosos
    \item $A_5$    : $A_5$ = \{3,4,...,M\}; es decir que más de dos remaches fueron defectuosos 
    \item $A_6$    : $A_6$ = \{t $|$ t $\leq$ 3\}; es decir la bombilla se quema en menos de 3 horas
    \item $A_{14}$ : $A_{14}$ = \{$(x,y) \ | \ y = x + 20$\}; es decir el máximo es 20º mayor que el mínimo
\end{itemize}
\vspace{0,5cm}

{Podemos ahora utilizar los diversos métodos para combinar conjuntos (es decir, eventos) y obtener nuevos:}

\begin{itemize}
    \item   Si A y B son eventos, $A\cup B$ es el evento que ocurre si y sólo si alguno de los eventos A o B ocurren.
    \item   Si A y B son eventos, $A\cap B$ es el evento que ocurre si y sólo si los eventos A y B ocurren.
    \item   Si A es un evento, $A^c$ es el evento que ocurre sí y solo sí el evento A no ocurre.
    \item   Si $A_1,A_2,...,A_n$ es cualquier colección finita de elementos, entonces $\bigcup_{i=1}^{n}A_i$ es el evento que ocurre sí y solo sí al menos un evento $A_i$ ocurre.
    \item   Si $A_1,A_2,...,A_n$ es cualquier colección finita de elementos, entonces $\bigcap_{i=1}^{n}A_i$ es el evento que ocurre sí y solo sí todos los eventos $A_i$ ocurren.
    \item   Si $A_1,A_2,...,A_n,...$ es cualquier colección infinita (numerable) de elementos, entonces $\bigcup_{i=1}^{\infty}A_i$ es el evento que ocurre sí y solo sí al menos un evento $A_i$ ocurre.
    \item   Si $A_1,A_2,...,A_n$ es cualquier colección infinita (numerable) de elementos, entonces $\bigcap_{i=1}^{\infty}A_i$ es el evento que ocurre sí y solo sí todos los eventos $A_i$ ocurren.
    \item  Supóngase que S representa el espacio muestral asociado al experimento $\epsilon$ y realizamos $\epsilon$ n veces. Entonces $S\ x\ S...\ x\ S$ se puede utilizar para representar todos los resultados de esas n repeticiones.Es decir, $(s_1,s_2,...,s_n) \in S\ x \ S\ x \ ...\ x\ S$ significa que $s_1$ resultó cuando se realizó el experimento por primera vez y $s_2$ cuando se realizó por segunda vez y así sucesivamente.
\end{itemize}
\vspace{0,5cm}

{{\bf Definición.} Se dice que dos eventos A y B son {\bf mutuamente excluyentes}, si no pueden ocurrir juntos. Expresamos esto escribiendo $A\cap B = \emptyset$; es decir, la intersección de A y B es el conjunto vacío.}
\vspace{0,2cm}
\begin{center}
{\bf Ejemplo página 14-15 (Meyer)}    
\end{center}

\vspace{0,2cm}

{Una de las características del concepto de experimento es que no sabemos que resultado particular se obtendrá al realizarlo. En otras palabras, si A es un evento asociado con el experimento, no podemos indicar con certeza que A ocurrirá o no. Por lo tanto, llega a ser muy importante la tarea de asignar un número al evento A que medirá, de alguna manera, la posibilidad de que el mismo ocurra. Lo cual nos lleva a la teoría de probabilidad ...}
\vspace{0,5cm}

\newpage

\vspace{0,5cm}

\subsection{Frecuencia Relativa}
\vspace{0,5cm}

{Dada n repeticiones de un experimento $\epsilon$ y sean A y B dos eventos asociados con $\epsilon$. Sean $n_A \ y \ n_B$ el número respectivo de veces que el evento A y el evento B ocurrieron en las n repeticiones.}
\vspace{0,5cm}

{{\bf Definición.} $f_A=\frac{n_A}{n}$ es la {\bf frecuencia relativa} del evento A en las n repeticiones del experimento $\epsilon$.}
\vspace{0,5cm}

{\bf Propiedades}
\vspace{0,2cm}

\begin{enumerate}
    \item  $0\leq f_A \leq 1$.
    \item  $f_A=1\iff$ A ocurre en cada una de las n repeticiones.
    \item  $f_A=0\iff$ A no ocurre en ninguna de las n repeticiones.
    \item  Sea $f_{A\cup B}$ la frecuencia relativa asociada al evento $A\cup B$ y sean A y B son eventos mutuamente excluyentes, entonces $ f_{A\cup B} = f_A + f_B$.
    \item Dada $f_A$ basada en n repeticiones de un experimento $\epsilon$, si $n \to \infty \Rightarrow f_A \to P(A) \footnote{Es un hecho empírico}$
\end{enumerate}
\vspace{0,2cm}
\begin{center}
{\bf Ejemplo página 16 (Meyer)}    
\end{center}
\vspace{0,2cm}

{Lo importante de la propiedad 5 es que si un experimento se realiza un gran número de veces, la frecuencia relativa con que ocurre un evento A tiende a variar cada vez menos a medida que el número de repeticiones aumenta. A esta característica se la denomina como {\bf regularidad estadística}.}
\vspace{0,5cm}

{{\bf Experimento:} es susceptible de estudiarse matemáticamente mediante un modelo no determinista cuando es posible efectuarlo repetidas veces y a causa de esto presente regularidad estadística.  }

\vspace{0,5cm}

\subsection{Nociones básicas de probabilidad}
\vspace{0,5cm}

{Con lo visto hasta ahora, la única forma de la cual podríamos obtener este preciado número asociado a un evento es por medio de la repetición del experimento un gran número de veces. Para evitar este proceso, se procede con la siguiente definición.}
\vspace{0,5cm}

{{\bf Definición.} Sea $\epsilon$ un experimento y S un espacio muestral asociado con $\epsilon$. A cada evento A le asociamos un número real P(A), llamado {\bf probabilidad de A}, el cual satisface las siguientes propiedades:}
\vspace{0,5cm}

\begin{enumerate}
    \item $0 \leq P(A) \leq 1$
    \item $P(S) = 1$
    \item Si A y B son eventos que se excluyen mutuamente $P(A\cup B) = P(A) + P(B)$
    \item Si $A_1,A_2,...,A_n,...$ son eventos que se excluyen mutuamente de par en par, entonces:
\end{enumerate}
\begin{center}
    $P(\bigcup_{i=1}^\infty A_i) = P(A_1) + P(A_2) + ... + P(A_n) + ...$
\end{center}
\vspace{0,5cm}

\newpage

{\subsubsection{Teorema 1.1}}
{Dado $\emptyset$, P($\emptyset$) = 0 {\bf (Dem. Pág. 19)}}
\vspace{0,5cm}


{\subsubsection{Teorema 1.2}}

{Sea $A^c$ el evento complementario de A, entonces $P(A^c) =  1 - P(A)$} {\bf (Dem. Pág. 19)}
\vspace{0,5cm}

{\subsubsection{Teorema 1.3}}
{Si A y B son eventos cualesquiera, entonces $P(A\cup B) = P(A) +P(B)-P(A\cap B)$} {\bf (Dem. Pág. 19)}
\vspace{0,5cm}

{\subsubsection{Teorema 1.4}}
{Si A, B y C son tres eventos cualesquiera, entonces $P(A\cup B\cup C) = P(A) +P(B)+P(C)-P(A\cap B)-P(A\cap C)-P(C\cap B)+P(A\cap B\cap C))$} {\bf (Dem. Pág. 20)}
\vspace{0,5cm}

{\subsubsection{Teorema 1.5}}
{Si $A \subset B$ entonces $P(A) \leq P(B)$} {\bf (Dem. Pág. 21)}
\vspace{1cm}

\subsection{Observaciones}
\vspace{0,5cm}    

\begin{itemize}
    \item Al elegir un modelo probabilístico no se descartan las relaciones deterministas. Lo importante es tener en consideración que cuando adoptamos un modelo probabilístico las condiciones con las cuales analizamos una situación pueden variar de forma imprecisa lo cual hace que nuestra variable en estudio fluctúe de manera imprecisa (por las relaciones determinísticas) y por lo tanto es significativo hablar de la probabilidad de tanto las condiciones y los valores de la variable observada.
    \item De acuerdo a la circunstancia podremos realizar suposiciones adicionales acerca de la conducta probabilística de nuestros resultados experimentales que nos conducirán a un método para evaluar las probabilidades básicas. A tener en cuenta, las suposiciones que realicemos sobre las probablidades debe ser tal que satisfagan los axiomas básicos presentados anteriormente. Un ejemplo es la frecuencia relativa; es una forma de aproximar la probabilidad de un evento, por lo tanto si la utilizamos como probabilidad estamos realizando una suposición.
\end{itemize}
\newpage

\subsection{El espacio muestral finito}
\vspace{0,5cm}

{Dado S = \{$a_1,a_2,...,a_k$\}; consideramos los eventos elementales, digamos por ejemplo E = \{$a_i$\}.}
\vspace{0,5cm}

{A cada uno de los eventos elementales \{$a_i$\} asignamos un número $p_i$, llamado la probabilidad de \{$a_i$\}, que satisface las condiciones siguientes:}
\vspace{0,2cm}

\begin{enumerate}
    \item $p_i \geq 0$, $i=1,2,...,k$
    \item $p_1+p_2+....+p_k = 1$.
\end{enumerate}
\vspace{0,5cm}

{Supongamos que un evento A está constituido por r resultados, $1\leq r \leq k$, digamos}
\vspace{0,2cm}

\begin{center}
    $A = \{a_1,a_2,...,a_r\}$
\end{center}
\vspace{0,2cm}
{Dado que $A = \bigcup_{i=1}^r\{a_i\} $ y los eventos $\{a_i\}$ son mutuamente excluyentes de a pares, por la propiedad 4:}
\vspace{0,2cm}

\begin{center}
    $P(A) = \sum_{i=1}^r P(\{a_i\})$
\end{center}
\vspace{0,2cm}

{Es decir, la probabilidad de un evento A se puede calcular como la suma de las probabilidades de los eventos elementales que conforman a A. Por supuesto que para evaluar las $p_i$ se deberán hacer algunas suposiciones respecto a los resultados individuales.}
\vspace{0,2cm}
\begin{center}
    \bf Ejemplo pág 28
\end{center}
\vspace{0,2cm}

\subsection{Resultados igualmente probables}
\vspace{0,5cm}

{La suposición que más comunmente se hace para espacios muestrales finitos es que todos los resultados son igualmente probables. De ninguna
manera esta suposición puede darse como un hecho; debe justificarse
con cuidado. Hay muchos experimentos para los cuales se garantiza tal
suposición, pero también hay muchas situaciones experimentales en las
cuales sería un error hacer tal suposición. Por ejemplo, sería muy poco
realista suponer que es tan probable no recibir llamadas telcfónicas en
una central cntre la 1 AM. y 2 AM. como entre las 5 PM y las 6 PM. }
\vspace{0,5cm}

{Sea S = \{$a_1,a_2,...,a_k$\} y sean los k resultados son igualmente probables; sabemos que :}
\vspace{0,2cm}

\begin{itemize}
    \item $p_1+p_2+....+p_k = 1$
    \item $p_1 = p_2 =....= p_k = x$
\end{itemize}

\vspace{0,2cm}

{Por lo tanto, $k*x=1 \Rightarrow \frac{1}{k} = x = p_i$. De esto se deduce que para cualquier evento A que conste de r resultados, tenemos}
\vspace{0,2cm}

\begin{center}
    $P(A)=\frac{r}{k}$
\end{center}
\vspace{0,2cm}

{Este modo de evaluar P(A) se suele indicar como}
\vspace{0,2cm}

\begin{center}
    $P(A)=\frac{cantidad\ de\ maneras\ en\ que\ \epsilon\ puede\ ocurrir\ favorable\ a\ A}{cantidad\ total\ de\ maneras\ en\ las\ que\ \epsilon\ puede\ ocurrir}$
\end{center}
\vspace{0,2cm}

{Esta forma de calcular P(A) es consecuencia de la suposición realizada en este apartado.}
\vspace{0,2cm}
\begin{center}
    \bf Ejemplos pág 29
\end{center}
\vspace{0,2cm}


\newpage

\subsection{Probabilidad condicional}
\vspace{0,5cm}

{Sean A y B dos eventos asociados con un experimento $\epsilon$. Indiquemos
con P(B $|$ A) la probabilidad condicional del evento B, dado que A ha
ocurrido. }
\vspace{0,2cm}

{Supongamos que un experimento $\epsilon$ se ha repetido n veces. Sean $n_A,n_B,n_{A\cap B}$ el número respectivo de veces que los eventos A, B y $A\cap B$ han ocurrido en las n repeticiones. Si planteamos la ecuación $\frac{n_{A\cap B}}{n_A}$ podríamos interpretarla como la frecuencia relativa de B entre esos resultados en los que A ocurrió; esto es la  {\bf frecuencia relativa condicional de B dado que A ocurrió}}
\vspace{0,2cm}

{Podemos escribir $\frac{n_{A\cap B}}{n_A}$ como sigue:}
\vspace{0,2cm}

\begin{center}
    $\frac{n_{A\cap B}}{n_A}$ = $\frac{(n_{A\cap B})/n}{{(n_A)}/{n}} = \frac{f_{A\cap B}}{f_A}$
\end{center}
\vspace{0,2cm}

{donde $f_{A\cap B}$ y ${f_A}$ son la frecuencia relativa de los eventos ${A\cap B}$ y ${A}$ respectivamente. Si el número n es lo suficientemente grande, $f_{A\cap B}$ tiende a $P({A\cap B})$, ${f_A}$ a P(A) y $\frac{n_{A\cap B}}{n_A}$ a $P(A|B)$.}
\vspace{0,8cm}
\begin{center}
    {\bf Definición.} {$P(A|B)=\frac{P(A\cap B)}{B}$, dado que $P(B) > 0$.}
\vspace{0,8cm}
\end{center}


{Cada vez que calculamos P(B $|$ A), esencialmente estamos calculando P(B) respecto al espacio muestral reducido A, en vez de espacio muestral original S.
Cuando calculamos P(B) nos preguntamos qué tan probable es que estemos en B,
sabiendo que debemos estar en S, y cuando
evaluamos P(B $|$ A) nos preguntamos qué
tan probable es que estemos en B, sabiendo
que debemos estar en A. (Esto es, el espacio muestral se ha reducido de S a A.) }
\vspace{0,2cm}
\begin{center}
    \bf Ejemplos ilustradores sobre el concepto págs. 43-45
\end{center}
\vspace{0,2cm}

{Tenemos dos maneras de calcular la probabilidad condicional P(A$|$B):}
\vspace{0,2cm}

\begin{enumerate}
    \item En forma directa considerando la posibilidad de A sobre el espacio muestral reducido B.
    \item Usando la definición anterior, donde $P(A\cap B) \ y \ P(A)$ se calculan respecto al espacio muestral original S.
\end{enumerate}
\vspace{0,2cm}

\begin{center}
    \bf Observaciones pág. 46, ejemplo 47.
\end{center}
\vspace{1cm}

\subsubsection{Teorema de multiplicación}
\vspace{0,5cm}

{La consecuencia más importante de la definición de probabilidad condicional anterior se obtiene escribiéndola como:}
\vspace{0,2cm}

\begin{center}
    $P(B|A).P(A) =P(A\cap B) = P(A|B).P(B)$\\
\end{center}
\vspace{0,2cm}

{Podemos aplicar este teorema al cálculo de la probabilidad de la ocurrencia simultánea de dos eventos A y B.}
\vspace{0,2cm}

\begin{center}
    \bf Ejemplo pág. 47
\end{center}
\vspace{0,2cm}

\newpage

Dado A y B, podemos hacer una afirmación general de la magnitud relativa de P($A|B$) y P(A):
\vspace{1cm}

{$P(A|B) ? P(A)$ = $\left \{ \begin{array}{lcc}
             P(A|B) = 0 \leq P(A)   & si &   A\cap B \ = \emptyset \\\
             \\ P(A|B) = \frac{P(A\cap B)}{P(B)} = \frac{P(A)}{P(B)} \geq P(A). \ Ya\ que\ 0\leq P(B) \leq 1 &  si & A\subset B\\
             \\P(A|B) = \frac{P(A\cap B)}{P(B)} = \frac{P(B)}{P(B)} = 1 \geq P(A) &  si  & B\subset A
             \\ \\ En\ este\ caso\ no\ se\ podra\ hacer\ ninguna\ afirmacion\ al\ respecto\ & si & otro\ caso
             \end{array}
   \right.$
   }
\vspace{1cm}

\subsubsection{Partición}
\vspace{0,5cm}

{Decimos que los eventos $B_1,B_2,...,B_k$ representan una partición en el espacio muestral S sí:}
\vspace{0,2cm}

\begin{itemize}
    \item $\forall_{i,j/i\not= j} \ B_i\cap B_j = \emptyset$
    \item $\bigcup_{i=1}^kB_i = S$
    \item $\forall_i\ P(B_i) > 0,$
\end{itemize}
\vspace{0,2cm}

{en otras palabras cuando se efectúa $\epsilon$, ocurre uno y sólo uno de los eventos $B_i$}.

\vspace{1,5cm}
\subsubsection{Teorema de la probabilidad total}
\vspace{1cm}

{Sea A algún evento respecto a S y sea $B_1,B_2,...,B_k$ una partición de S. Podemos escribir a A como:}
\vspace{0,2cm}

\begin{center}
    $A = A\cap B_1 \ \cup\ A\cap B_2\ \cup\ ...\ \cup\ A\cap B_k$,
\end{center}

{donde las parejas $A\cap B_1, \ A\cap B_2,\ ...\ , A\cap B_k$, son mutuamente excluyentes y por lo tanto aplicando la propiedad aditiva:}
\vspace{0,2cm}

\begin{center}
    $P(A) = P(A\cap B_1) + P(A\cap B_2) + ... + P(A\cap B_k)$,
\end{center}

{cada termino $P(A\cap B_j)$ se puede expresar como $P(A|B_j)\times P(B_j)$, quedándonos de esta forma el teorema de la probabilidad total:}
\vspace{,2cm}

\begin{center}
    $P(A) = P(A|B_1)\times P(B_1) + P(A|B_2)\times (B_2) + ... + P(A|B_k)\times (B_k)$.
\end{center}
\vspace{0,5cm}
\begin{center}
    \bf Ejemplos págs. 50-51
\end{center}
\vspace{0,2cm}


\newpage

\subsection{Teorema de Bayes}
\vspace{1cm}

{Sean $B_1,B_2,...,B_k $ una partición del espacio muestral S y A un evento asociado con S. Si queremos calcular la probabilidad condicional $P(B_i|A)$ tenemos que:}
\vspace{0,5cm}

\begin{center}
    $P(B_i|A) = \frac{P(B_i\cap A)}{P(A)}$
\end{center}
\vspace{0,2cm}

{Substituyendo por iguales (definición de probabilidad condicional y teorema de la probabilidad total) esta ecuación nos queda como:}
\vspace{0,2cm}

\begin{center}
    $P(B_i|A) = \frac{P(A|B_i)\times P(B_i)}{\sum_{j=1}^kP(A|B_j)\times P(B_j)}$ $i = 1,2,...,k.$
\end{center}
\vspace{0,5cm}

{Este resultado se conoce como el teorema de Bayes. También se la llama fórmula para la probabilidad de las causas. Puesto que las $B_i$ son una partición del espacio muestral, uno y solo uno de los eventos $B_i$ ocurre. Por lo tanto, la fórmula anterior nos da la probabilidad de un $B_i$ particular (esto es una causa), dado que el evento A ha ocurrido. Para aplicar este teorema debemos conocer todos los valores P($B_i$).}
\vspace{0,5cm}
\begin{center}
    \bf Ejemplos págs. 52-53
\end{center}
\vspace{1cm}

\subsection{Eventos independientes}
\vspace{0,5cm}

{\bf Definición.} {Dos eventos A y B son eventos independientes si y sólo si:}
\vspace{0,3cm}
\begin{center}
    $P(A\cap B) = P(A)\times P(B)$,
\end{center}
\vspace{0,3cm}

{la idea de eventos independientes surje de pensar que $P(A|B) = P(A)$ y a su vez $P(B|A) = P(B)$ }
\vspace{0,5cm}

\begin{center}
    \bf Ejemplos 54-57
\end{center}
\vspace{0,5cm}

{\bf Definición.} {Decimos que tres eventos A, B y C son mutuamente independientes  si y sólo si todas las condiciones siguientes se mantienen:}
\vspace{0,3cm}
\begin{center}
    $P(A\cap B) = P(A)\times P(B)$,\\
    $P(A\cap C) = P(A)\times P(C)$,\\
    $P(B\cap C) = P(B)\times P(C)$,\\
    $P(A\cap B\cap C) = P(A)\times P(B)\times P(c)$.
\end{center}
\vspace{0,3cm}

{Esta noción se generaliza a n eventos mutuamente independientes.}
\vspace{0,5cm}
\begin{center}
    \bf Ejemplos págs. 58-60
\end{center}
\vspace{0,5cm}


\end{document}