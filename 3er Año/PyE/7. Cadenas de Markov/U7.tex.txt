\documentclass{report}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{vmargin}
\usepackage{graphicx}

\usepackage{etoolbox}

\graphicspath{ {images/} }
\setpapersize{A4}
\setmargins{2.5cm}       % margen izquierdo
{1.5cm}                        % margen superior
{16.5cm}                      % anchura del texto
{23.42cm}                    % altura del texto
{10pt}                           % altura de los encabezados
{1cm}                           % espacio entre el texto y los encabezados
{0pt}                             % altura del pie de página
{2cm}           
\begin{document}

\begin{titlepage}
\centering
{\bfseries\LARGE Universidad Nacional de Rosario \par}
\vspace{0,5cm}
{\scshape\Large Facultad de Ciencias Exactas, Ingeniería y Agrimensura \par}
\vspace{2cm}
{\scshape\Huge Probabilidad y Estadística\par}
\vspace{3cm}
{\itshape\Huge Unidad 7 \par}
\vfill
{\Large Autor del resumen:\par}
\vspace{1 cm}
{\Large Charles Chaplin \par}
\vfill
{\Large Julio 2020 \par}
\end{titlepage}

\newpage


\chapter{Cadenas de Markov : Primeros pasos}


\vspace{1cm}

\section{Introduccion}

\vspace{1cm}

{Consideren un juego con un tablero que se base en 10 casillas cuadradas (numeradas 1-10) dispuestas en circulo. (Un Monopoly en miniatura.) Un jugador empieza en la casilla 1. A cada turno, el jugador tira un dado y se mueve alrededor del tablero las cantidad casillas que aparezca en la cara del dado. El jugador se sigue moviendo alrededor de las casillas de acuerdo a la tirada de dado (Esta garantizado que este no es un juego muy exitante...)

Ahora bien, sea $X_k$ el numero de la casilla en el cual el jugador se encuentra luego de k movimientos, con $X_0 = 1$. Supongamos que el jugador obtiene las siguientes tiradas 2,1 y 4. Las primeras cuatro posiciones son:}

\[(X_0,X_1,X_2,X_3) = (1,3,4,8).\]

{Dada esta informacion, ¿Qué podemos decir acerca de el próximo casillero ($X_4$) que ocupara el jugador? A pesar de que sabemos el historial completo de tiradas del jugador, la única información relevante para predecir su posición en el futuro es la ubicación más reciente (en este caso $X_3$). Ya que $X_3 = 8$ entonces necesariamente $X_4 \in A=\{9,10,1,2,3,4\}$ y cada resultado posible tiene la misma probabilidad. Formalmente}

\[\ \forall_{j \in A} \  P(X_4=j|X_0=1,X_1=3,X_2=4,X_3=8)= P(X_4=j|X_3=8) = \frac{1}{6}.\]

{Dada la posición más actual del jugador $X_3$, su futura posición $X_4$ es independiente del pasado de la historia $X_0,X_1,X_2$. \\ \\

La secuencia de posiciones del jugador $X_0,X_1,X_2,...$ es un proceso estocástico llamado {\bf Cadena de Markov}. Este juego ilustra una muy importante propiedad de la cadena de Markov: {\bf El futuro, dado el presente, es independiente del pasado} }
\vspace{0,5cm}

{\bf Cadena de Markov.} {Sea S un conjunto discreto. Una cadena de Markov es una secuencia  de variables aleatorias $X_0,X_1,...$, cuyos espacios muestrales son S con la siguiente propiedad: Dado $n\geq 0$}

\[\forall_{i,j \in S} \ P(X_{n+1}=j|X_0=x_0,...,X_{n-1} = x_{n-1},X_n = i) = P(X_{n+1} = j | X_n =i)\]

{El conjunto S es el {\bf Espacio de estados} de la cadena de Markov. Generalmente nos referimos a $X_n = i$ como que la cadena visitó la posición i en n pasos.}

\newpage

{Una cadena de Markov es {\bf homogénea en el tiempo} si las probabilidades mencionadas anteriormente no dependen de n. Esto es}

\[\forall_{n\geq0} \ P(X_{n+1} = j | X_{n} = i) = P(X_1 = j|X_0 = i)\]

{Dado que las probabilidades anteriores dependen solamente de i y j, pueden ser almacenadas en una matriz {\bf P} cuya ij-ésima entrada corresponde a $P_{ij} = P(X_1 = j | X_0 = i)$. A esta matriz se la llama {\bf matriz de transición} o {\bf matriz de Markov}, que contiene las probabilidades de moverse de un estado actual a cualquier otro (lo que se conoce como "probabilidad de un paso"). Si el espacio de estados tiene k elementos, entonces la matriz de transición tiene $k\times k$. Si el espacio de estados es infinito numerable, la matriz de transición es infinita.}
\vspace{0,5cm}

{Las entradas de todas las matrices de transición de Markov son no-negativas y cada fila suma 1,}

\[\forall_{fila \  i}\ \sum_jP_{ij} = \sum_j P(X_1 = j | X_0 = i) = \sum_j \frac{P(X_1=j,X_0=i)}{P(X_0=i)}=\frac{P(X_0=i)}{P(X_0=i)} = 1\]

\begin{center}
    \bf Ejemplo p.42 Dobrow
\end{center}
\vspace{0,5cm}

{\bf Matriz Estocástica.} {Una matriz estocástica es una matriz cuadrada, que satisface:}

\begin{itemize}
    \item [1.] $\forall_{ij}\ P_{ij} \geq 0 $
    \item [2.] Para cada fila i, $\sum_j Pij=1.$
\end{itemize}
\vspace{0,5cm}

\begin{center}
    \bf Ejemplos p.42-52
\end{center}
\vspace{1cm}
\section{Cálculos basicos}
\vspace{2cm}

{Una poderosa característica de las Cadenas de Markov es la habilidad de usar matrices para computar probabilidades. Para usar los métodos de matriz, consideramos a las distribuciones de probabilidades como vectores. Un vector de probabilidad es un vector de números no-negativos cuya suma es igual a 1. Generalmente se denotan con letras griegas en negrita, como por ejemplo {\bf $\alpha, \ \lambda, y \ \pi$}. \\

Supongamos que X es una variable aleatoria discreta con P(X=j) = $\alpha_j$, para j = 1,2,... . Luego {\bf $\alpha$}=$(\alpha_1,\alpha_2,...)$ es un vector de probabilidad. Decimos que la distribución de X es $\alpha$. \\

Para el cálculo matricial identificaremos distribución de probabilidades discretas como vectores fila.\\

Para una cadena de Markov $X_0,X_1,...,$ la distribución de $X_0$ es llamada {\bf distribución inicial de la cadena de Markov}. Si $\alpha$ es la distribución inicial, entonces P($X_0$=j)=$\alpha_j$, para toda j.}

\newpage

\subsection{Probabilidad de transición en n pasos}
\vspace{1cm}

{Para los estados i y j, con n$\geq$1, $P(X_n=j|X_0=i)$ es la probabilidad de que la cadena que comenzó en i llegue a j en n pasos. La probabilidad de transición en n pasos puede ser acomodada en una matriz. La matriz cuya ij-ésima entrada es $P(X_n=j|X_0=i)$ es la matriz de transición en n pasos de la cadena de Markov. Por supuesto, para n=1 es la usual matriz de transicion P. Para $n\geq1$, uno de los resultados centrales del cálculo de cadenas de Markov es que la matriz de transicion en n pasos es precisamente $P^n$, la enésima potencia de P.}
\vspace{0,5cm}
\begin{center}
    \bf D/ p.53
\end{center}
\vspace{1cm}

{\bf Matriz de transición en n pasos.}
{Sea $X_0,X_1,...$, una cadena de Markov con una matriz de transicion {\bf P}. La matriz $P^n$ es la matriz de transición en n pasos de la cadena. Para $n\geq0$,}

\[\forall_{ij} \ P_{ij}^n = P(X_n = j | X_0=i)\]

{\bf Observacion.} {Note que $P^n_{ij}=(P^n)_{ij}$. No hay que confundirse con $(P_{ij})^n$. También note que $P^0$ es la matriz identidad. Esto es}
\vspace{0,3cm}

\[P_{ij}^0=P(X_0=j|X_0=i) = \left\{ \begin{array}{lcc}
             1 &   si  & i = j \\
             \\ 0 &  si  & i \not= j
             \end{array}
   \right.\]
\vspace{0,5cm}
\begin{center}
    \bf Ejemplos p.54-55
\end{center}   
\vspace{0,5cm}
  
\subsection{Relación Chapman–Kolmogorov}
\vspace{2cm}

{Para m,n $\geq$0, la identidad matricial $P^{n+m}= P^m P^n$ nos da como resultado que }

\[\forall_{i,j} \ P^{n+m}_{ij}=\sum_k P^m_{ik}P^n_{kj}\]

\begin{center}
    \bf D. p.55
\end{center}
\vspace{0,5cm}

{La interpretación probabilística es que realizando una transición desde i hacia j en m+n pasos es equivalente a transicionar desde i a algún estado k en m pasos y después transicionar desde este último estado a j en los restantes n pasos. Esto es conocido como {\bf la relación Chapman-Kolmogorov} }

\newpage

\subsection{Distribución de $X_n$}
\vspace{1cm}

{En general, una cadena de Markov $X_0,X_1,...,$ no es una secuencia de variables aleatorias igualmente distribuidas. Para n $\geq$ 1, la distribución marginal de $X_n$ depende en el n-ésimo paso con su matriz de transición $P^n$, asi como de la distribución inicial $\alpha$. Para obtener la función de probabilidad de $X_n$, dado que ocurre $X_0$,}

\[\forall_j \ P(X_n=j)=\sum_iP(X_n=j|X_0=i)P(X_0=i)=\sum_iP^n_{ij}\alpha_i.\]

{La suma anterior puede ser interpretada como el producto punto del vector de probabilidad inicial $\alpha$ con la j-ésima columna de $p^n$. Es decir, es el j-ésimo componente del vector-matriz resultante del producto $\alpha P^n$}

\vspace{0,4cm}

\begin{center}
    \bf Ejemplo p.56
\end{center}
\vspace{1cm}

\subsection{Presente, futuro y pasado más reciente}
\vspace{1cm}

{La propiedad de Markov dice que el pasado y el futuro son independientes dado el presente. También es verdad que el pasado y el futuro son independientes, dado el pasado más reciente}
\vspace{0,5cm}

{\bf Propiedad de Markov}
\vspace{0,3cm}

{Sea $X_0,X_1,...$ una cadena de Markov. Luego, para toda $m<n$,}
\[P(X_{n+1}=j|X_0=i_0,...,X_{n-m-1}=i_{n-m-1},X_{n-m}=i)\]
\[= P(X_{n+1}=j|X_{n-m}=i)= P(X_{m+1}=j|X_{0}=i) = P^{m+1}_{ij}\]

{Para toda $i,j,i_0,...,i_{m-n-1}, n\geq 0$}
\vspace{0,3cm}
\begin{center}
    \bf D. p.57
\end{center}
\vspace{1cm}

\subsection{Distribución conjunta}
\vspace{1cm}
{La distribución marginal de una cadena de Markov está determinada por la distribución inicial $\alpha$ y la matriz de transición P. De hecho, $\alpha$ y P determinan todas las distribuciones conjuntas de una cadena de Markov, esto es, la probabiludad conjunta de cualquier subconjunto finito de $X_0,X_1,...$. En ese sentido, la distribución inicial y la matriz de transición dan una completa descripción probabilística de una cadena de Markov.} 
\vspace{0,3cm}

{Para ilustrar consideremos una probabilidad conjunta arbitraria como:}

\[P(X_5=i,X_6=j,X_9=k,X_{17}=l), \ para \ algunos \ estados \ i,j,k,l. \]

\newpage

{Para el evento previamente mencionado, la cadena se mueve a i en 5 pasos, luego a j en un paso, luego a k en tres pasos y luego a l en 8 pasos. Con una distribución inicial $\alpha$, la intuición nos dice que}

\[P(X_5=i,X_6=j,X_9=k,X_{17}=l)= (\alpha P^5)_iP_{ij}P^3_{jk}P^8_{kl}. \]

{Y de hecho, la probabilidad condicional, la propiedad de Markov y la homogeneidad del tiempo nos dan}
\begin{itemize}
    \item []
    \item [ ] $P(X_5=i,X_6=j,X_9=k,X_{17}=l)$
    \item []
    \item [=] $P(X_{17}=l|X_5=i,X_6=j,X_9=k) P(X_9=k|X_5=i,X_6=j) P(X_6=j|X_5=i) P(X_5=i)$ 
    \item []
    \item [=] $P(X_{17}=l|X_9=k) P(X_9=k|X_6=j) P(X_6=j|X_5=i) P(X_5=i)$ 
    \item []
    \item [=]  $P(X_{8}=l|X_0=k) P(X_3=k|X_0=j) P(X_1=j|X_0=i) P(X_5=i)$ 
    \item []
    
    \item [=] $P^8_{kl}P^3_{jk}P_{ij}(\alpha P^5)_i$
\end{itemize}
\vspace{0,5cm}

{La probabilidad conjunta es obtenida apartir de la distribución inicial $\alpha$ y la matriz de transición P. Para completitud, aquí dejamos la formula general }
\vspace{0,5cm}

{\bf Probabilidad conjunta}
{Sea $X_0,X_1,...,$ una cadena de Markov con matriz de transición P y distribución inicial $\alpha$. Para todo $0\leq n_1 \leq n_2 \leq ... \leq n_{k-1} \leq n_k$ y estados $i_1,i_2,...,i_{k=1},i_k$,}

\[P(X_{n1}=i_i,X_{n2}=i_2,...,X_k=i_k) = (\alpha P^{n_1})_{i_1} (P^{n_2-n_1})_{i_1i_2} ... (P^{n_k-n_{k-1}})_{i_{k-1}i_k}\]
\vspace{0,5cm}
\begin{center}
    \bf Ejemplo p.59
\end{center}
\vspace{0,5cm}

\subsection{Comportamiento a largo plazo, la evidencia numérica}
\vspace{1cm}

{En cualquier proceso estocástico o determinista, el comportamiento a largo plazo del sistema es, muchas veces, de interés. Evidencia numérica sugiere que la potencia de una matriz converge a un límite. Lo que es más, las filas de esta matriz llevada al límite son iguales. El hecho de que las filas de una cierta matriz $p^n, n\rightarrow \infty$ son iguales significa que la probabilidad de una potencia k en particular (para un k en el cual ya se puede observar esta propiedad), no depende del estado actual. Después de cierto tiempo, el estado inicial se diluye y no afecta a la distribución de la potencia de la matriz. }
\vspace{0,5cm}

\begin{center}
    \bf Ejemplos. p59-65 y Simulaciones p.65-68
\end{center}

\newpage

\chapter{Cadenas de Markov a largo plazo}
\vspace{1cm}

\section{Distribucion limite}
\vspace{1cm}

{En muchos casos, una cadena de Markov presenta un comportamiento limite a largos plazos. La cadena se decanta a una distribución equilibrada, la cual es independiente de su estado inicial.}
\vspace{0,3cm}

{\bf Distribución límite.\\}
{Sea $X_0,X_1,...$ una cadena de Markov con una matriz de transición P. Una distribución límite de la cadena de Markov es una distribución de probabilidad $\lambda$ con la propiedad que}

\[\forall_{i,j} \ lim_{n\rightarrow \infty}P(X_n=j)= \lambda_j\]

{En otras palabras:}
\begin{itemize}
    \item [(i)] Para cualquier distribucion inicial y para todo j
    
        \[lim_{n\rightarrow \infty}P(X_n=j)=\lambda_j\]
        
    \item [(ii)] Para cualquier distribucion inicial $\alpha$ 
        \[lim_{n\rightarrow \infty}\alpha P^n= \lambda\]
        
    \item [(iii)] Sea $\Lambda$ la matriz estocastica cuyas filas son iguales a $\lambda$.
    
    \[lim_{n\rightarrow \infty} P^n= \Lambda\]
    
\end{itemize}
\vspace{1cm}

{Interpretaremos $\lambda_j$ como la probabilidad a largo plazo de que la cadena se encuentre en el estado j. Por la unicidad del límite, si una cadena de Markov tiene una distribución límite, entonces esta distribución es única. Un método rapido y sucio de verificar de que una distribución límite existe es aplicar grandes potencias a la matriz inicial hasta que de una de ellas se obtenga una matriz límite con filas iguales. }

\vspace{0,3cm}
\begin{center}
    \bf Ejemplo p.77-78
\end{center}

\newpage

\section{Proporción del tiempo en cada estado}
\vspace{1cm}

{La distribución límite nos presenta la probabilidad a plazos largos de que una cadena de Markov se encuentre en cada estado. También puede ser interpretada como la proporción del tiempo, a plazos largos, de la cadena visitando cada estado.}
\vspace{0,3cm}
\begin{center}
    \bf Planteo y ejemplos p.78-80
\end{center}
\vspace{1cm}

\section{Distribución estacionaria}
\vspace{1cm}

{Es interesante considerar qué pasa si asignamos la matriz límite de una cadena de Markov a ser la distribucion inicial de la cadena. {\bf Planteo p. 80-81}}
\vspace{0,5cm}

{\bf Distribución estacionaria.\\}
{Sea $X_0,X_1,...$ una cadena de Markov con una matriz de transición P. Una distribución estacionaria es una distribución de probabilidades $\pi$ que satisface}

\[\pi = \pi P.\]

Esto es

\[\forall_j \ \pi_j = \sum_i \pi_iP_{ij}\]

{Si asumimos que una distribución estacionaria $\pi$, es la distribución inicial, entonces todas las $X_n$ tienen la misma distribución:}

\[\pi P^n =(\pi P)P^{n-1} = \pi P^{n-1} ...= \pi P = \pi\]
\vspace{0,5cm}

{Si la distribución inicial es una distribución estacionaria, entonces $X_0,X_1,...$ es una secuencia de variables aleatorias idénticamente distribuidas. Esto no implica que las variables aleatorias son independientes. Al contrario, la estructura dependiente entre las variables aleatorias sucesivas en la cadena de Markov está gobernada por la matriz de transición independientemente de la distribución inicial.\\

Nos referimos a la cadena de Markov estacionaria o a la cadena de Markov en estado estacionario cuando la cadena alcanzó su distribución estacionaria. Si una cadena de Markov tiene una distribución límite entonces esa distribución es una distribución estacionaria.\\

Hay que tener en cuenta que hay cadenas de Markov con más de una distribución estacionaria, también aquellas que no tienen distribuciones límites y aquellas que tienen una única distribución estacionaria que no son distribuciones límites.}

\newpage

\section{Matrices de transición regulares}
\vspace{1cm}

{Una matriz de transición P es llamada regular si alguna potencia de P es positiva. Esto es, $P^n >0$, para algún $n\geq1$}
\vspace{0,3cm}

\begin{center}
    \bf Ejemplo p.83
\end{center}

{Si una matriz de transición de una cadena de Markov es regular, entonces la cadena tiene una distribución límite, la cual es la única distribución estacionaria de la cadena.}
\vspace{0,3cm}

{\bf Teorema del límite para cadenas de Markov regulares.\\}

{Una cadena de Markov cuya matriz de transición P es regular tiene una distribución límite, la cual es única, positiva y distribución estacionaria de la cadena. Esto es, existe un vector de probabilidad unico $\pi>0$, tal que}

\[lim_{n\rightarrow \infty}P^n_{ij}=\pi_j\]
para todo i,j donde

\[\sum_i\pi_iP_ij=\pi_j\]

Equivalentemente, existe una matriz estocastica positiva $\Pi$ tal que

\[lim_{n\rightarrow \infty}P^n=\Pi\]

Donde $\Pi$ tiene todas las filas iguales a $\pi$ y $\pi$ es el unico vector de probabilidad que satisface

\[\pi P=\pi\]
\vspace{0,5cm}

\begin{center}
    \bf Ejemplo p.84
\end{center}
\vspace{0,5cm}

{Una forma de decir si una matriz estocástica no es regular es que si para alguna potencia n, todos los 0s en $P^n$ aparecen en los mismo lugares que todos los 0s en $P^{n+1}$, luego estos apareceran en los mismos lugares para todas las potencias mayores, y la matriz no es regular.}
\vspace{1cm}

\begin{center}
    \bf Ejemplo p.85
\end{center}

\vspace{0,5cm}


\newpage



\section{Buscando la distribución estacionaria.}
\vspace{1cm}
{Supongamos que $\pi$ es una distribución estacionaria para una cadena de Markov con una matriz de transición P. Entonces,}

\[\sum_i \pi_iP_{ij}=\pi_j, \ para \ todos \ los \ estados \ j \]

{Lo que nos da un sistema de ecuaciones lineales. Si P es una matriz $k \times k $, el sistema tiene k ecuaciones y k incógnitas. Ya que las filas de P suman 1, el sistema $k \times k $ contendrá una ecuación redundante.\\}

{Para la cadena de dos estados general, con}

\begin{equation}
   P= \begin{pmatrix}
    {1-p} & p \\
    q     & {1-q} 
    \end{pmatrix}
\end{equation}

{Las ecuaciones son}

\begin{itemize}
    \item [] \[(1-p)\pi_1+q\pi_2=\pi_1\]
    \item [] \[p\pi_1+(1-q)\pi_2=\pi_2\]
\end{itemize}

{Las ecuaciones son redundantes y conducen a $\pi_1p=\pi_2q$. Si p y q son distintos a cero, entonces junto a la condicion $\pi_1+\pi_2=1$, la única solución es }

\[\pi=\left(\frac{q}{p+q},\frac{p}{p+q}\right)\]
\vspace{0,5cm}
\begin{center}
    \bf Ejemplo p.86
\end{center}

{Una técnica útil para encontrar distribuciones estacionarias, la cual reduce el número de ecuaciones a resolver en uno. Esta técnica usa el hecho de que si x es una vector, no necesariamente un vector de probabilidad, que satisfaga xP=x, entonces (cx)P=cx para todas las constasntes c. Si uno puede encontrar un vector x no negativo, que satisfaga xP=x, entonces un único vector de probabilidad $\pi=cx$ puede ser obtenido a partir de una decisión apropiada de c de tal forma que las filas de cx sumen 1. En particular, sea c=$\frac{1}{\sum_jx_j}$, el recíproco de la suma de los componentes de x\\}

{El sistema lineal $\sum_i \pi_iP_{ij}=\pi_j$ sin la restricción $\sum_i \pi_i=1$, tiene una ecuación redundante. Nuestro método de solución consiste en (i) eliminar la ecuación redundante y (ii) resolver el sistema resultante para $x=(1,x_2,x_3,...)$, donde la primer componente (o cualquier otra) es reemplazada por 1. Para una cadena de Markov con k estados, este metodo reduce el problema a resolver un sistema lineal $(k-1)\times (k-1)$.\\

Si la cadena original tenia una única distribucion estacionaria, entonces el sistema lineal reducido tendrá una única solución, pero una la cual no es necesariamente un vector de probabilidad. Para hacerla vector de probabilidad cuyos componentes sumen 1, hay que dividir por la suma de los componentes. En otras palabras, la única distribución estacionaria es }

\[\pi = \frac{1}{1+x_2+x_3+...+x_k}(1,x_2,...,x_k)\]
\vspace{0,3cm}

\begin{center}
    \bf Ilustración y ejemplos p.87-90
\end{center}

\newpage

\section{Clases de comunicación}
\vspace{1cm}

{El comportamiento a largo plazo de una cadena de Markov esta relacionada a cuan frecuentemente los estados son visitados. Aqui observaremos de cerca a la relación entre estados y cuan alcanzables, o accesibles, grupos de estados son entre sí.}
\vspace{0,5cm}



{Digamos que el estado j es {\bf accesible} desde el estado i si  $P^n_{ij}>0$, para algún $n\geq0$. Esto es, hay una probabilidad positiva de alcanzar j desde i en una cantidad finita de pasos. Los estados i y j se {\bf comunican} si i ees accesible desde j y j es accesible desde i.  La {\bf comunicación} es una relación de equivalencia, lo que significa que satisface las siguientes tres propiedades:}

\begin{itemize}
    \item [a)] {\bf Reflexiva.} Todos los estados se comunican con si mismos
    \item [b)] {\bf Simetrica.} Si i se comunica con j entonces j se comunica con i
    
    \item [c)] {\bf Transitiva.} Si i se comunica con j y j se comunica con k entonces i se comunica con k
\end{itemize} 


{\bf Observaciones.}
{La propiedad $a)$ se debe a que $P^0_{ii} = P(X_0=i|X_0=i)=1.$ La propiedad}{ $b)$ se debe a que la definición de comunicación es simétrica. Para la propiedad }{$c)$, supongamos que i se comunica con j y que j se comunica con k. Entonces, existen $n \geq 0$ y $m\geq 0$ tal que $P^n_{ij} >0$ y $P^m_{jk} >0$. Por lo tanto }

    \[P^{n+m}_{ik} = \sum_t P^n{it}P^m_{tk} \geq P^n_{ij}P^m_jk > 0 \]

{Lo que nos da como resultado que k es accesible desde i e i es accesible desde k}
\vspace{0,5cm}

{Dado que la comunicación es una relación de equivalencia el espacio de estados puede ser particionado en clases de equivalencias, llamadas {\bf clases de comunicación}. Esto es, el espacio puede ser dividido entre subconjuntos disjuntos, en los cuales todos los estados pertenecientes a una clase estan intercomunicados pero no se comunican con ningún estado fuera de la misma. Un grafo de transición modificado es una herramienta útil para encontrar las clases de comunicacion de una cadena de Markov. Los vértices del grtafo son los estados de la cadena. Una arista direccionada es dibujada entre i y j si $P_{ij} >0$.}
\vspace{0,5cm}

\begin{center}
    \bf Ejemplo p.95
\end{center}
\vspace{0,5cm}

{\bf Irreducibilidad.} {Cuando una cadena de Markov tiene una sola clase de comunicación, esta es {\bf irreducible.}}
\vspace{0,5cm}

\begin{center}
    \bf Ejemplo p.95
\end{center}
\vspace{1cm}
\newpage

\section{Recurrencia y transitoriedad}
\vspace{1cm}

\begin{center}
    \bf Ejemplo ilustrador p.95-96
\end{center}
\vspace{0,3cm}

{Dada una cadena de Markov $X_0,X_1,...,$ y sea $T_j=min\{n>0:X_n=j\}$ la primera vez que la cadena se situa en el estado j. Si $X_n \not= j,$ para toda $n>0$, el conjunto $T_j = \infty$. Sea}

\[f_j=P(T_j<\infty|X_0=j)\]

{la probabilidad de que la cadena que comienza en j eventualmente retorne a j}
\vspace{0,5cm}

{\bf Estados de recurrencia y de transicion.} {Un estado j es llamado recurrente si la cadena de Markov iniciada en j revisita j. Esto es, $f_j=1$.
Un estado k es llamado de transicion si hay una probabilidad positiva de que la cadena de Markov iniciada en k nunca retorne a k. Esto es, $f_k<1$.}
\vspace{0,5cm}

{Si un estado es eventualmente revisitado o no está estrechamente relacionado con cuan seguido ese estado es visitado. Para la cadena que inicia en i, sea}

\[I_n = \left\{ \begin{array}{lcc}
             1 &   si  & X_n=j, \\
             \\ 0 &  si & X_n\not=j \\
             \end{array}
   \right.,\]
   
  {para $n\geq0$. Luego, $\sum_{n=0}^\infty I_n$ es el número de visitas a j. El número esperado de visitas a j es}
  
  \[E\left(\sum_{n=0}^\infty I_n\right)=\sum_{n=0}^\infty E(I_n) =\sum_{n=0}^\infty P(X_n=j|X_0=i)= \sum_{n=0}^\infty P^n_{ij}\]
  \vspace{0,3cm}
  
 {donde la suma infinita puede divergir a $\infty$. Para la cadena iniciada en i, el  número esperado de visitas a j es la ij-esima entrada de la matriz $\sum_{n=0}^\infty P^n$.}
 \vspace{0,5cm}
 
 {Supongamos que j es recurrente. La cadena que inicia en j volvera eventualmente a visitar j con probabilidad 1. Una vez que alcance 1, la cadena comienza denuevo y se comporta como una nueva version de la cadena que comienza en j. Decimos que la cadena de Markov se {\bf regenera} asi misma. (Este comportamiento intuitivo se conoce como la propiedad fuerte de Markov.) Desde j, la cadena re-visitara j denuevo, con probabilidad 1, y asi sucesivamente. A esto le sigue que j sera revisitada una cantidad infinita de veces y}
 \[\sum_{n=0}^\infty P^n_{jj} = \infty.\]
 
 \vspace{0,5cm}
 
 {Por otro lado, supongamos que j es de transicion. Comenzando en j, la probabilidad de eventualmente alcanzar j denuevo es $f_j$, y la probabilidad de nunca alcanzar j es 1-$f_j$. Si la cadena alcanza j, el evento que volvera eventualmente a visitar j es independiente de la historia pasada. A esto le sigue que la secuencia de visitas sucesivas a j se comporta como una secuencia de tiradas de moneda independientes e igualmente distribuidas donde cara ocurre si se llege eventualmente a j y cruz ocurre si j nunca es alcanzada denuevo. El numero de veces que j es alcanzada es el numero de tiradas de moneda hasta que cruz ocurra, el cual tiene una distribucion geometrica con parametro 1-$f_j$. Asi, el numero esperado de visitas a j es  $\frac{1}{1-f_j}$ y }
 
  \[\sum_{n=0}^\infty P^n_{jj} = \frac{1}{1-f_j} <\infty.\]
 
 {En particular un estado de transicion sera visitado solo un numero finito de veces.}

\newpage
 
 {\bf Recurrencia y transitoriedad}
 \begin{itemize}
     \item [(i)] El estado j es recurrente si y solo si
     
    \[\sum_{n=0}^\infty P^n_{jj} = \infty.\]
    
    \item [(ii)] El estado j es de transicion si y solo si
    
    \[\sum_{n=0}^\infty P^n_{jj} <\infty.\]
 \end{itemize}
  \vspace{0,5cm}
  
 {Supongamos que j es recurrente y accesible desde i. Como la cadena empezo en i hay una probabilidad positiva de alcanzar j. Y desde j, el numero esperado de visitas a j es infinito. A esto le sigue que el numero esperado de visitas a j para la cadena iniciada en i es tambien infinito y asi}
  
    \[\sum_{n=0}^\infty P^n_{ij} = \infty.\]
    \vspace{0,5cm}
    
{Supongamos que j es de transicion y accesible desde i. Por un argumento similar el numero de visitas a j para la cadena iniciada en i es finita y asi}
   \[\sum_{n=0}^\infty P^n_{ij} <\infty.\]
   
desde el cual le sigue que

\[lim_{n\rightarrow \infty}P^n_{ij} =0.\]

{\bf La probabilidad a largo plazo de que una cadena de Markov eventualmente se pose sobre un estado de transicion es 0. }
\vspace{0,5cm}

{\bf Recurrencia y transitoriedad son propiedades de clase.}
{Los estados de una clase de comunicacion son todos recurrentes o todos de transicion.}
\begin{center}
    \bf D/p.99
\end{center}
\vspace{0,5cm}

{\bf Corolario.} {Para una cadena de Markov finita e irreducible, todos los estados son recurrentes.}
\begin{center}
    \bf D/p.99
\end{center}
\vspace{0,5cm}

\begin{center}
    \bf Ejemplo p.100-101
\end{center}

\newpage

\section{Descomposicion Canonica.}
\vspace{1cm}
{Un conjunto de estados C se dice cerrado si ningun esatado fuera de C es accesible desde cualquier estado en C. Si C es cerrado, entonces $P_{ij=0}$ para todo $i\in C$ y $j \notin C$}
\vspace{0,5cm}

{\bf Clase de comunicacion cerrada.} {Una clase de comunicacion es cerrada si consiste de todos estados recurrentes. Una clase de comunicacion finita es cerrada solo si consiste de estados recuerrentes.}
\vspace{0,5cm}

\begin{center}
    {\bf D/101}
\end{center}
\vspace{0,5cm}

{El espacio $S$ de una cadena de Markov finita puede ser particionada en estados de transicion y de recurrencia de la siguiente manera, $S=T\cup R_1 \cup R_2 \cup ... \cup R_m$, donde T es el conjunto de todos los estados de transicion y los $R_i$ son clases de comunicacion cerrada de estados de recurrencia. Este proceso es llamado descomposicion canonica. La computacion de muchas cantidades asociadas con cadenas de Marvos puede ser simplificada por esta descomposicion. Dada una descomposicion canonica, el espacio de estados puede ser reordenado de manera tal que la matriz de transicion de Markov tiene la siguiente forma de matriz en bloques }



\[P=\bordermatrix{~ & T & R_1             & \hdots          & R_m \cr
                T       & *      & *      & \hdots          & * \cr
                R_1     & 0      & P_1    & \hdots          & 0 \cr
                \vdots  & \vdots & \vdots & \ddots & \vdots\cr
                R_m & 0 &  0     & \hdots          & P_m}\]



{Cada submatriz $P_1,...,P_m$ es una matriz estocastica cuadrada que corresponde a una clase de comunicacion cerrada. Por si sola, cada una de esas matrices es una cadena de Markov irreducible con un espacio de estados reducido.}
\vspace{0,5cm}

\begin{center}
    {\bf Ejemplo 102}
\end{center}
\vspace{0,5cm}

{La descomposicion canonica es util para describir el comportamiento a largo plazo de una cadena de Markov. La forma de bloque de la matriz facilita obtener la potencia de las matrices. Para $n \geq 1$.}


\[P^n=\bordermatrix{~ & T & R_1   & \hdots & R_m \cr
                T   & * & *       & \hdots & * \cr
                R_1 & 0 & P^n_1   & \hdots & 0 \cr
                \vdots  & \vdots  & \vdots & \ddots & \vdots\cr
                R_m & 0 &  0      & \hdots & P^n_m}\]
{Tomando limites obtenemos}

\[lim_{n\rightarrow\infty}P^n=
\bordermatrix{~ & T    & R_1    & \hdots & R_m \cr
                T      & 0      & *      & \hdots & * \cr
                R_1    & 0      & P^n_1  & \hdots & 0 \cr
                \vdots & \vdots & \vdots & \ddots &\vdots\cr
                R_m    & 0 &  0    & \hdots & P^n_m}\]

{Notese que las entradas de las columnas correspondientes a estados de transicion estan todas en 0, como consecuencia de lo planteado anteriormente.}
\vspace{0,3cm}

{Las clases de comunicacion recurrentes y cerradas $R_1,...,R_m$ se comportan como mini irreducibles cadenas Markov donde todos los estados se comunican entre si. Las propiedades asintocticas de las submatrices $P_1,...,P_m$ nos guian a considerar las propiedades de cadenas de Markov irreducibles.}
\vspace{0,5cm}

\section{Cadenas de Markov irreducibles}
\vspace{1cm}

{\bf Teroema del limite para cadenas de Markov irreducibles.\\}
{Suponga que $X_0,X_1,..$ es una cadena de Markov irreducible. Para cada estado j, sea $\pi_j=E(T_j|X_0=j)$ sea el tiempo esperado para retornar a j. Entonces $\pi_j$ es finito, y existe una unica y positiva distribucion estacionaria $\pi$ tal que }

\[\forall_j \pi_j = \frac{1}{\mu_j}\]

{Lo que es mas, para todos los estados i,}

\[\pi_j=lim_{n\rightarrow \infty} \frac{1}{n} \sum_{m=0}^{n-1} P_{ij}^m.\]

{El teorema anterior caracteriza la distribucion estacionaria $\pi$ para cadenas de Markov finitas e irreducibles. Relaciona la probabilidad estacionaria $\pi_j$ a el numero esperado de pasos entre visitar j. Recordemos que $T_j= min \{n>0:X_n=j\}$ es el tiempo minimo que tarda en llegar al estado j. }
\vspace{01cm}

{\bf Observaciones}
\begin{itemize}
    \item [1] El hecho de que $\pi_j = \frac{1}{\mu_j}$ es intuitivo. Si hay una visita a j cada $\mu_j$ pasos, entonces la proporcion de visitas a j es $\frac{1}{\mu_j}$.
    
    \item [2] El teorema no afirma que $\pi$ es una distribucion limite. La convergencia en la ultima ecuacion es una forma debil de convergencia respecto a $\pi_j=lim_{n\rightarrow\infty}P^n_{ij}$. Discutiremos mas adelante que se debe hacer una suposicion adicional a $\pi$ para ser una distribucion limite.
    
    \item [3]  Para cadenas de Markov finitas e irreducibles, todos los estados son recurrentes y el tiempo esperado $E(T_j|X_0=j)$ es finito, para todo j. Sin embargo, para una cadena de Markov infinita si j es un estado recurrente, a pesar de que la cadena eventualmente revisitara j con probabilidad 1, el numero experado de pasos entre estas visitas no necesariamente son finitos. El teorema puede extenderse a cadenas de Markov infinitas e irreducibles para la cual el tiempo esperado de retorno $E(T_j|X_0=j)$ es finito, para todo j. Un estado recurrente j es llamado recurrente positivo si $E(T_j|X_0=j) < \infty$, y nulo recurrente si $E(T_j|X_0=j)=\infty$. Asi, el teorema se sostiene para cadenas de Markov irreducibles para la cual todos sus estados son positivos recurrentes.
\end{itemize}

{En muchas aplicaciones, el tiempo esperado entre visitas a un estado en particular es de importancia.}
\vspace{0,5cm}
\begin{center}
    \bf Ejemplos. 104-105
\end{center}

{\bf Analisis del primer paso.\\}
{El tiempo esperado de retorno $E(T_j|X_0=j)$ es obtenido apartir del reciproco de la probabilidad estacionaria $\pi_j$. Otro acercamiento es condicionar en el primero paso de la cadena y usar la ley de la expectativa total. Este es llamado {\bf analisis del primer paso}.}
\vspace{0,5cm}
\begin{center}
    \bf Ejemplos. 105-106
\end{center}

\newpage

\section{Periodicidad}
\vspace{1cm}

{Cadenas de Markov finitas e irreducibles tiene una unica distribucion estacionaria positiva. A pesar de que es posible que no tengan distribucion limite, ellas tienen al menos un comportamiento limite en el sentido de que para todos los estados i y j, el promedio parcial $\frac{1}{n}\sum_{m=0}^{n-1}P^n_{ij}$ converge.}
\vspace{0,5cm}

\begin{center}
    \bf Ejemplo ilustrador 107
\end{center}
\vspace{0,5cm}

{Recordar que el divisor comun mas grande (gcd) de un conjunto de enteros positivos es el entero mas grande qeu divide a todo los numeros del conjunto de manera tal que el resto sea 0}
\vspace{0,1cm}

{\bf Periodo.} {Para una cadena de Markov con una matriz de transicion P, el periodo del estado i, denotado como d(i), es el divisor comun mas grande del conjunto de posibles tiempos de retornos a i. Esto es,}

\[d(i) = gcd \{n > 0 :P^n_{ii} > 0\}.\]

{Si d(i) = 1 , el estado i es llamado {\bf a-periodico}. Si el conjunto de tiempos de retorno al nodo i es vacio, el conjunto $d(i)=\infty$.}
\vspace{0,5cm}

{\bf La periocidad es una propiedad de clase.} {Los estados en una clase de comunicacion tienen el mismo periodo.}
\vspace{0,5cm}

\begin{center}
    \bf D/ p.108. Ejemplos p.108-109
\end{center}
\vspace{0,5cm}

{\bf Cadenas de Markov  periodicas y aperiodicas.} {Una cadena de Markov es llamada periodica si es irreducible y todos los estados tienen un periodo mayor a 1. Una cadena de Markov es aperiodica si es irreducible y todos los estados tienen periodo igual a 1.}
\vspace{0,3cm}

{Notar que cualquier estado i con la propiedad que $P_{ii} > 0$ es necesariamente aperiodico. Asi, una condicion suficiente para una cadena de Markov irreducible para ser aperiodica es que $P_ii >0$ para algun i. Esto es, al menos una entrada diagonal de la matriz de transicion es distinta a 0.}
\vspace{0,3cm}

\section{Cadenas de Markov ergodicas.}

{Una cadena de Markov es llamada ergodica si es irreducible, aperiodica y todos los estados tienen esperanza de retornar en el tiempo finitas. Lo siguiente es siempre verdadero para cadenas finitas. Asi, una cadena de Markov finita es ergodica si es irreducible y aperiodica. Precisamente la clase de cadenas de Markov ergodicas tienen distribucion limite positiva.}
\vspace{0,5cm}

{\bf Teorema fundamental del limite para cadenas de Markov ergodicas}

{Sea $X_0,X_1,...,$ una cadena de Markov ergodica. Existe una unica y positiva distribucion estacionaria $\pi$, la cual es la distribucion limite de la cadena. Esto es,}

\[\pi_j = lim_{n\rightarrow \infty}P^n_{ij}, \ para \ toda \ i,j\]

{Este teorema afirma el mismo resultado del limite para cadenas de Markov con matrices de transicion regulares. La demostracion esta dada en la seccion 3.10, donde tambien se demuestra que una cadena de Markov es ergodica si y solo si su matriz de transicion es regular. }
\begin{center}
    \bf Ejemplos p.110-114
\end{center}
\newpage

\section{Reversibilidad del tiempo}
\vspace{1cm}

{Algunas cadenas de Markov exhiben un comportamiento direccional en su evolucion. La propiedad de reversibilidad del tiempo puede ser explicada intuitivamente como sigue. Si pudieses ver un video de la cadena de Markov moviendose hacia adelante en el tiempo y luego correr el video hacia atras, no podrias decir la diferencia entre estos dos. }
\vspace{0,3cm}

{\bf Reversibilidad del tiempo.} {Una cadena de Markov irreducible con matriz de transicion P y distribucion estacionaria $\pi$ es reversible o reversible en el tiempo si}

\[\pi_iP_{ij} =\pi_jP_{ji}, \ para \ toda \ i,j.\]

{Las ecuaciones anteriores son llamadas ecuaciones de equilibrio detalladas. Ellas dicen que para una cadena  en estado estacionario, }

    \[P(X_0=i,X_1=j) = P(X_0=j,X_1=i), \ para \ todos \ i,j.\]
    

{Esto es, la frecuencia de transiciones de i a j es igual a la frecuencia de transiciones de j a i. Mas generalmente, si una cadena de Markov estacionaria es reversible entonces}

\[P(X_0=i_0,X_1=i_1,...,X_n=i_n)=P(X_0=i_n,X_1=i_{n-1},...,X_n=i_0), \ para \ toda \ i_0,i_1,...,i_n.\]


{Si la distribucion estacionaria de una cadena de Markov es uniforme, entonces por la equacion descripta anteriormente la cadena es reversible si la matriz de transicion es simetrica.}
\vspace{0,3cm}

\begin{center}
    \bf Ejemplos p.115-119
\end{center}
\vspace{1cm}

\section{Cadenas absorbentes}
\vspace{1cm}

\begin{center}
    \bf Serpientes y Escaleras. Ejemplo ilustrador.
\end{center}
\vspace{0,5cm}

{\bf Estado absorbente, cadena absorbente.} {El estado i es un estado absorbente si $P_{ii}=1.$ Una cadena de Markov es llamada cadena absorbente si tiene al menos un estado absorbente.}
\vspace{0,3cm}

{Consideremos una cadena de Markov absorbente con k estados de los cuales t estados son de transicion y k-t son absorbentes. Estos estados pueden ser reordenados, como en la descomposicion canonica, con su matriz de transicion escrita bloques de la siguiente manera,}
\vspace{0,5cm}

\[P=\left( \begin{tabular}{r | l}
   Q  & R \\ \hline
    0 & I \\ 
\end{tabular} \right)\]
\vspace{0,5cm}

{Donde Q es una matrix $t \times t$, R es una matriz $t\times(k-t)$, 0 es una matriz $(k-t) \times t$ de 0s y  I es una matriz identidad $(k-t) \times (k-t)$}.

\newpage

{Computar potencias de P es facilitado por la forma en bloque de la matriz. Tenemos}
\vspace{0,3cm}

\[P^2=\left( \begin{tabular}{r | l}
   Q  & R \\ \hline
    0 & I \\ 
\end{tabular} \right)  \left( \begin{tabular}{r | l}
   Q  & R \\ \hline
    0 & I \\ 
\end{tabular} \right) = \left( \begin{tabular}{r | l}
   Q^2  & (I+Q) R \\ \hline
    0 & I \\ 
\end{tabular} \right)\]
\vspace{0,3cm}

{En general para $n\geq 1$ tenemos}
\vspace{0,3cm}

\[P^n=\left( \begin{tabular}{r | l}
   Q^n  & (I+Q+...+Q^(n-1)) R \\ \hline
    0 & I \\ 
\end{tabular} \right)\]
\vspace{0,3cm}

{\bf Lema. Sea A una matriz cuadrada con la propiedad que $A^n \rightarrow 0$, cuando $n \rightarrow \infty.$ Entonces,}

\[\sum_{n=0}^\infty A^n= (I-A)^{-1}.\]

{\bf D/ p.122-123}.
\vspace{0,3cm}

{Ahora podremos calcular $lim_{n\rightarrow \infty}P^n$,}

\[lim_{n\rightarrow \infty}P^n=

= lim_{n\rightarrow \infty} \left( \begin{tabular}{r | l}
   Q^n  & (I+Q+...+Q^{(n-1)}) R \\ \hline
    0 & I \\ 
\end{tabular} \right)  

=\left( \begin{tabular}{r | l}
  lim_{n\rightarrow \infty} Q^n  & lim_{n\rightarrow \infty}(I+Q+...+Q^{(n-1)}) R \\ \hline
    0 & I \\ 
\end{tabular} \right) 

= \left( \begin{tabular}{r | l}
   0  & (I-Q)^{-1} R \\ \hline
    0 & I \\ 
\end{tabular} \right)\]

{Consideremos la interpretacion de $ (I-Q)^{-1} R$. Esta matriz es indexada por filas transitorias y columnas absorbentes. La ij-esima entrada es la probabilidad a largo plazo que una cadena iniciada en un estado de transicion i sea absorbida en el estado j. Si la cadena de Markov tiene solo un estado absorbente, esta submatriz sera una $(k-1)$- vector columna de 1s }

\vspace{0,3cm}

\begin{center}
    \bf Ejemplos p.123-125
\end{center}
\vspace{0,5cm}

{\bf Numero esperado de visitas a estados de transicion.} {Consideremos una cadena de Markov absorbente con t estados de transicion. Sea F una matriz $t\times t$ indexada por estados de transicion, donde $F_{ij}$ es el numero esperado de visitas a j dado que la cadena comienza en i. Entonces,}

\[F =(I-Q)^{-1}\]

{La matriz F es llamada matriz fundamental. \bf D/125-126}

\vspace{1cm}
\newpage
\section{Tiempo esperado de absorcion.}
\vspace{1cm}

{Para una cadena de Markov absorbente iniciada en un estado de transicion i, sea $a_i$ el tiempo de absorcion esperado, el numero de pasos esperados para alcanzar algun estado de abosrcion. El numero de transiciones desde i hasta un estado de absorcion es simplemente la suma del numero de transiciones desde i hacia cada uno de los estados de transicion hasta que eventualmente llegue a un estado de absorcion. El numero esperado de pasos desde i hacia un estado de transicion j es $F_{ij}$. A esto le sigue que,}

\[a_i=\sum_{k\in T}F_{ik}\]

{En forma de vector, $a=F1$ donde 1 es el vector columna de todos 1s. Esto es, el tiempo de absorcion esperado son la suma de las filas de la matriz fundamental.}
\vspace{0,3cm}

{\bf Cadenas de Markov absorbentes.} {Para una cadena de Markov con todos los estados de transicion o absorbentes, sea $F=(I-Q)^{-1}$,}
\begin{itemize}
    \item [1.] (Probabilidad de absorcion) la probabilidad de que desde un estado de transicion i la cadena sea absorbida en el estado j es $(FR)_{ij}$
    
    \item [2.] (Tiempo de absorcion) El numero de pasos esperados desde un estado de transicion i hasta que la cadena sea absorbida en algun estado absorbente es $(F1)_i$.
\end{itemize}
\vspace{0,3cm}
\begin{center}
    \bf Ejemplos 127-128
\end{center}
\vspace{1cm}

\section{Tiempo esperado para cadenas irreducibles}
\vspace{1cm}

{ Para una cadena de Markov irreducible, la primera vez que la cadena llega a un estado puede ser analizado como tiempos de absorcion para una cadena adecuadamente modificada. En particular, asumamos que P es la matriz de transicion de una cadena de Markov irreducible. Para encontrar el tiempo esperado hasta que el estado i sea alcanzado por primera vez, consideremos una nueva cadena en la cual i es un estado de absorcion. La matriz de transicion $\tilde{P}$ para la nueva cadena es obtenida haciendo 0 la i-esima fila de la matriz P y la i-esima columna de P. El tiempo que la cadena original P llega a i es igual a el tiempo de que la cadena modificada $\tilde{P}$ es absorbida en i.}
\vspace{0,3cm}
\begin{center}
    \bf Ejemplos 129-... (sigue propiedad fuerte de Markov y demas cosas complementarias.)
\end{center}
\newpage

\section{Procesos de Poisson}
\vspace{1cm}

{\bf Introduccion}
\vspace{0,5cm}

{Mensajes de textos arrivando a tu celular a tiempos irregulares durante el dia. Accidentes que ocurren en una avenida en tiempos y lugares aparentemente aleatoriamente distribuidos. Bebes nacidos en momentos fortruitos en un centro de maternidad. Todos estos fenomenos son modelados por el proceso de Poisson, un proceso estocastico usado para modelar la ocurrencia, o llegada, de eventos en un intervalo continuo. Tipicamente, el intervalo representa tiempo. Un proceso de Poisson es un tipo especial de proceso de conteo.\\
Dado una corriente de eventos que arrivan en tiempos aleatorios empezando en t=0, sea $N_t$ el numero de llegadas que ocurren para el tiempo t, esto es, el numero de eventos en [0,t]. Por ejemplo, $N_t$ podria ser el numero de mensajes de textos recibidos hasta el tiempo t. }
\vspace{0,3cm}

{Para cada $t\geq0,N_t$ es una variable aleatoria. La coleccion de variables aleatorias $(N_t)t\geq 0$, es un proceso estocastico en tiempo continuo de valor entero, llamado proceso de conteo. Ya que $N_t$ cuenta eventos en [0,t], a medida que t aumenta, los numeros de eventos $N_t$ tambien lo hacen.}
\vspace{0,5cm}

{\bf Proceso de conteo.} {Un proceso de conteo $(N_t)t\geq0$ es una coleccion de variables aleatorias enteras y no negativas tal que si $ 0 \leq s \leq t,$ entonces $N_s\leq N_t$.}
\vspace{0,5cm}

{A diferencia de una cadena de Markov, la cual es una secuencia de variables aleatorias, un proceso de conteo conforma una coleccion incontable, ya que esta indexada sobre un intervalo de tiempo continuo.}
\vspace{0,5cm}

{El trayecto de un proceso de conteo es una funcion escalonada continua por derecha. Si $0 \leq s < t$, entonces $N_t - N_s$ es el numero de eventos en el intervalo (s,t].}
\vspace{0,3cm}

\begin{center}
    \bf Grafico ilustrativo p.224
\end{center}
\vspace{0,3cm}

{Hay varias maneras para caracterizar el proceso de Poisson. Una puede centrarse en (i) el numero de eventos que occurren en intervalos fijos, (ii) cuando un evento ocurre, y el tiempo entre esos eventos, o (iii) el comportamiento probabilistico de eventos individuales en intervalos infinitesimales. Lo cual nos guia a tres definiciones de un proceso de Poisson, donde cada una ofrece puntos de vistas iluminadores para el modelo estocastico. }
\vspace{1cm}

{\bf Proceso de Poisson, definicion 1.}
\vspace{0,3cm}

{Un proceso de Poisson con parametro $\lambda$ es un proceso de conteo $(N_t)t\geq 0$ con las siguientes propiedades:}

\begin{itemize}
    \item [1.] $N_0 = 0$
    \item [2.] Para todo $t>0$, $N_t$ tiene una distribucion de Poisson con parametro $\lambda t$.
    \item [3.] {\bf Incrementos estacionarios.} Para toda s,t $>0, \ N_{t+s} - N_s $ tiene la misma distribucion que $N_t$. Esto es, 
    
    \[P(N_{t+s}-N_s=k)= P(N_t=k)=\frac{e^{-\lambda t}(\lambda t)^k}{k!}, \ para \ k=0,1,2...\]
    
    \item [4.] {\bf Incrementos independientes.} Para $0 \leq q < r \leq s < t, \ N_t-N_s$ y $N_r-N_q$ son variables aleatorias independientes.
\end{itemize}
\newpage

{La propiedad del incremento estacionario dice que la distribucion del numero de llegadas a un intervalo depende solamente de la longitud del mismo.
\vspace{0,3cm}

La propiedad del incremento independiente dice que el numero de llegadas en intervalos de tiempo disjuntos son variables aleatorias independientes.}
\vspace{0,3cm}

{Ya que $N_t$ tiene una distribucion de Poisson, $E(N_t)=\lambda t$. Esto es, esperamos alrededor de $\lambda t$ llegadas en t unidades de tiempo. Por esto, la proporcion de llegadas es $E(N_t)/t=\lambda$.}

\begin{center}
    \bf Ejemplos p.225-226
\end{center}
\vspace{0,5cm}

{\bf Proceso de Poisson transladado.}
\vspace{0,5cm}

{Sea $(N_t) t\geq 0$ un proceso de Poisson con parametro $\lambda$. Para un tiempo fijo $s>0$, consideren el proceso transladado $(N_{t+s} - N_s)t\geq0$. El proceso transladado es probabilisticamente equivalente al del proceso original.}
\vspace{0,5cm}

{\bf El proceso de poisson transladado es un proceso de Poisson.}
\vspace{0,3cm}

{Sea $(N_t)t\geq 0$ un proceso de Poisson con parametro $\lambda$. Para $s > 0,$ sea

\[N'_t = N_{t+s}, para t \geq 0\]

Luego, $(N'_t)t\geq 0$ es un proceso de Poisson con parametro $\lambda$.
}
\vspace{0,3cm}

{Tenemos que $(N')t\geq0$ es un proceso de conteo con $N'_0 = N_s - N_s = 0.$ Con aumentos estacionarios, $N'_t$ tiene la misma distribucion que $N_t$. Y el nuevo proceso hereda incrementos estacionarios e independientes del original. A esto le sigue que si $N_s = k$, la distribucion $N_{t+s}$ es igual a la distribucion de $N_t$}
\vspace{0,5cm}

\begin{center}
    \bf Ejemplo p.227
\end{center}
\vspace{0,5cm}
\subsection{ La llegada, los tiempos entre-llegadas.}
\vspace{1cm}

{Para un proceso de Poisson con parametro $\lambda$, sea X el tiempo de la primer llegada. Entonces, $ X > t$ si y solo si no hay llegadas en [0,t]. Ademas, }

            \[P(X > t) = P(N_t = 0) = e^{-\lambda t}, \ para \ t>0.\]
         
{Por lo tanto, X tiene una distribucion exponencial con parametro $\lambda$. La distribucion exponencial juega un rol central en el proceso de Poisson. Lo que es verdadero para el tiempo de la primer llegada lo es tambien para el tiempo entre la primer y segunda llegada, y para todos los tiempos entre-llegadas. Un proceso de Poisson es un proceso de conteo para el cual los tiempos entre-llegadas son independientes e identicamente distribuidas variables aleatorias exponenciales.}   
\vspace{0,5cm}
\newpage

{\bf Proceso de Poisson, definicion 2.}
\vspace{0,3cm}

{Sean $X_1,X_2,...,$ una secuencia de variables aleatorias exponenciales idependientes e identicamente distribuidas con parametro $\lambda$. Para $t>0$, sea}

    \[N_t = max \{n:X_1+...+X_n \leq t\}\],
    
{con $N_0 = 0$. Then, $(N_t)_{t\geq0}$ define un proceso de Poisson con parametro $\lambda$. Sea}

\[S_n = X_1+...+X_n, \ para \ n =1,2,...\]
 
Llamaremos a $S_1,S_2,...$ los tiempos de llegada del proceso, donde $S_k$ es el tiempo de la k-esima llegada. Lo que es mas,

\[X_k = Sk-S_{k-1}, \ para \ k=1,2,...\]

es el tiempo entre-llegada entre la (k-1)-esima y k-esima llegada, donde $S_0 =0$.
\vspace{0,3cm}

\begin{center}
    \bf Grafico ilustrador p.228-229
\end{center}
\vspace{0,5cm}

{\bf Simular}
\vspace{0,3cm}

{Un beneficio de la definicion 2 es que nos guia a un metodo directo para construir y simular un proceso de Poisson:}

\begin{itemize}
    \item [1.] Let $S_0=0.$
    \item [2.] Generar variables aleatorias exponenciales independientes e identicamente distribuidas $X_1,X_2,...$
    \item [3.] Sea $S_n= X_1+...+x_n$, para n = 1,2,...
    \item [4.] Para cada k = 0,1,..., sea $N_t=k$, para $S_k \leq t  < S_{k+1}$
\end{itemize}

\vspace{0,5cm}

{\bf No tiene memoria.}
\vspace{0,3cm}

{La distribucion exponencial es la unica distribucion continua que es falta de memoria. Una variable aleatoria X tiene la propiedad faltarle memoria si para toda, $s,t>0$,}

\[P(X>s+t|X>s) = P(X>t)\]

\begin{center}
    \bf Ilustracion, ejemplos y el minimo de variables aleatorias exponenciales independientes p.229-232
\end{center}
\vspace{0,5cm}


\end{document}

